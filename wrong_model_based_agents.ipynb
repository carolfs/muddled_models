{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reproduces Figure 1 from the paper. It simulates three types of purely model-based agents: correct, transition-dependent learning rates (TDLR), and unlucky symbol. The data are analyzed and the results are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "from os.path import join\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "from utils import *\n",
    "\n",
    "# Number of trials each simulated agent will perform\n",
    "NUM_TRIALS = 1000\n",
    "# Model parameters: learning rate, inverse temperature, and\n",
    "# second-stage value reduction caused by choosing the \"unlucky symbol\"\n",
    "ALPHA, BETA, ETA = 0.5, 5.0, 0.5\n",
    "# Different learning rates for the TDLR model\n",
    "ALPHA_COMMON, ALPHA_RARE = 0.8, 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct model-based strategy is defined below, for comparison with the other strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mb_correct(alpha, beta):\n",
    "    \"\"\"Simulation of a correct model-based strategy.\"\"\"\n",
    "    value = np.zeros((2, 2))\n",
    "    rwrd_probs = create_random_rwrd_probs()\n",
    "    choice1 = 0\n",
    "    for _ in range(NUM_TRIALS):\n",
    "        r = 0.4*(max(value[1]) - max(value[0]))\n",
    "        prob1 = expit(beta*r)\n",
    "        choice1 = int(random.random() < prob1)\n",
    "        fstate = get_random_fstate(choice1)\n",
    "        choice2 = int(random.random() < expit(beta*(value[fstate, 1] - value[fstate, 0])))\n",
    "        reward = get_random_reward(fstate, choice2, rwrd_probs)\n",
    "        value[fstate, choice2] = (1 - alpha)*value[fstate, choice2] + alpha*reward\n",
    "        rwrd_probs = diffuse_rwrd_probs(rwrd_probs)\n",
    "        yield (choice1, fstate, choice2, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the \"unlucky symbol\" algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mb_unlucky_symbol(alpha, beta, eta):\n",
    "    \"\"\"Simulation of the unlucky symbol algorithm.\"\"\"\n",
    "    value = np.zeros((2, 2))\n",
    "    rwrd_probs = create_random_rwrd_probs()\n",
    "    choice1 = 0\n",
    "    for _ in range(NUM_TRIALS):\n",
    "        r = 0.7*max(value[1]) + 0.3*max(value[0]) - eta*(0.3*max(value[1]) + 0.7*max(value[0]))\n",
    "        prob1 = expit(beta*r)\n",
    "        choice1 = int(random.random() < prob1)\n",
    "        fstate = get_random_fstate(choice1)\n",
    "        value_reduction = eta if choice1 == 0 else 1\n",
    "        choice2 = int(random.random() < expit(value_reduction*beta*(value[fstate, 1] - value[fstate, 0])))\n",
    "        reward = get_random_reward(fstate, choice2, rwrd_probs)\n",
    "        value[fstate, choice2] = (1 - alpha)*value[fstate, choice2] + alpha*reward\n",
    "        rwrd_probs = diffuse_rwrd_probs(rwrd_probs)\n",
    "        yield (choice1, fstate, choice2, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the transition-dependent learning rates (TDLR) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def mb_tdlr(alpha_common, alpha_rare, beta):\n",
    "    \"\"\"Simulation of the transition-dependent learning rates model-based strategy.\"\"\"\n",
    "    value = np.zeros((2, 2))\n",
    "    rwrd_probs = create_random_rwrd_probs()\n",
    "    choice1 = 0\n",
    "    for _ in range(NUM_TRIALS):\n",
    "        r = 0.4*(max(value[1]) - max(value[0]))\n",
    "        prob1 = expit(beta*r)\n",
    "        choice1 = int(random.random() < prob1)\n",
    "        fstate = get_random_fstate(choice1)\n",
    "        choice2 = int(random.random() < expit(beta*(value[fstate, 1] - value[fstate, 0])))\n",
    "        reward = get_random_reward(fstate, choice2, rwrd_probs)\n",
    "        if choice1 == fstate: # Common transition\n",
    "            alpha = alpha_common\n",
    "        else:\n",
    "            alpha = alpha_rare\n",
    "        value[fstate, choice2] = (1 - alpha)*value[fstate, choice2] + alpha*reward\n",
    "        rwrd_probs = diffuse_rwrd_probs(rwrd_probs)\n",
    "        yield (choice1, fstate, choice2, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below simulates the three types of agents below and calculates their stay probabilities and model-based weights. **Note that here we only simulate 100 agents, but for the paper we simulated 1000 agents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fractions import Fraction\n",
    "\n",
    "class CouldNotFitException(Exception):\n",
    "    pass\n",
    "\n",
    "def fit_model(stan_model, model_dat):\n",
    "    \"\"\"Fits a Stan model to data to get the maximum likelihood parameters.\"\"\"\n",
    "    log_lik = -np.inf\n",
    "    params = None\n",
    "    errors = 0\n",
    "    for _ in range(10):\n",
    "        while True:\n",
    "            try:\n",
    "                op = stan_model.optimizing(data=model_dat, as_vector=False, iter=5000)\n",
    "            except RuntimeError:\n",
    "                errors += 1\n",
    "                if errors > 100:\n",
    "                    sys.stderr.write('Could not fit model to data\\n')\n",
    "                    raise CouldNotFitException\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        if op['value'] > log_lik:\n",
    "            log_lik = op['value']\n",
    "            params = op['par']\n",
    "    return params\n",
    "\n",
    "class StayProbabilitiesCalculator:\n",
    "    \"\"\"Helper class for the stay probabilities.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.prev_reward = None\n",
    "        self.prev_transition = None\n",
    "        self.prev_choice = None\n",
    "        self.stay = [(0, 0) for _ in range(4)]\n",
    "    def add_trial_info(self, choice1, fstate, _, reward):\n",
    "        if self.prev_choice is not None:\n",
    "            indx = 2*self.prev_reward + self.prev_transition\n",
    "            num, den = self.stay[indx]\n",
    "            self.stay[indx] = (num + int(self.prev_choice == choice1), den + 1)\n",
    "        self.prev_reward = reward\n",
    "        self.prev_transition = int(choice1 == fstate)\n",
    "        self.prev_choice = choice1\n",
    "    def get_stay_prob(self, reward, transition):\n",
    "        num, den = self.stay[2*reward + transition]\n",
    "        return num/den\n",
    "\n",
    "# Directory where the hybrid model's Stan implementation is saved\n",
    "MODELS_DIR = 'models'\n",
    "# Parameters of the hybrid model\n",
    "HYBRID_PARAMS = ('alpha1', 'alpha2', 'lmbd', 'beta1', 'beta2', 'w', 'p')\n",
    "\n",
    "MODELS = (mb_correct, mb_unlucky_symbol, mb_tdlr)\n",
    "MODELS_PARAMS = {\n",
    "    mb_correct: (ALPHA, BETA),\n",
    "    mb_unlucky_symbol: (ALPHA, BETA, ETA),\n",
    "    mb_tdlr: (ALPHA_COMMON, ALPHA_RARE, BETA),\n",
    "}\n",
    "MODEL_LABELS = {\n",
    "    mb_correct: 'Correct',\n",
    "    mb_unlucky_symbol: 'Unlucky symbol',\n",
    "    mb_tdlr: 'TDLR',\n",
    "}\n",
    "\n",
    "def simulate_mb_agents(num_agents, print_progress=True):\n",
    "    \"\"\"Simulates model-based agents, then calculates stay probabilities and hybrid model parameters.\"\"\"\n",
    "    stan_model = get_stan_model(\n",
    "        join(MODELS_DIR, 'hybrid_no_log_lik.stan'),\n",
    "        join(MODELS_DIR, 'hybrid_no_log_lik.bin'))\n",
    "    cols = HYBRID_PARAMS + ('stay_prob_11', 'stay_prob_10', 'stay_prob_01', 'stay_prob_00')\n",
    "    model_results = {model: None for model in MODELS}\n",
    "    for model, params in MODELS_PARAMS.items():\n",
    "        if print_progress:\n",
    "            print('Simulating {} agents...'.format(MODEL_LABELS[model]))\n",
    "        results = []\n",
    "        agents = 0\n",
    "        while agents < num_agents:\n",
    "            if agents%(num_agents//100) == 0:\n",
    "                print(round(agents*100/num_agents, 1), end='% ')\n",
    "            model_dat = {}\n",
    "            s2 = []\n",
    "            rewards = []\n",
    "            a1 = []\n",
    "            a2 = []\n",
    "            spc = StayProbabilitiesCalculator()\n",
    "            for choice1, fstate, choice2, reward in model(*params):\n",
    "                # We sum 1 to final state, choice1 and choice2\n",
    "                # because the hybrid model expects 1/2 rather than 0/1.\n",
    "                s2.append(fstate + 1)\n",
    "                rewards.append(reward)\n",
    "                a1.append(choice1 + 1)\n",
    "                a2.append(choice2 + 1)\n",
    "                spc.add_trial_info(choice1, fstate, choice2, reward)\n",
    "            model_dat['s2'] = s2\n",
    "            model_dat['reward'] = rewards\n",
    "            model_dat['a1'] = a1\n",
    "            model_dat['a2'] = a2\n",
    "            model_dat['T'] = NUM_TRIALS\n",
    "            try:\n",
    "                fitted_params = fit_model(stan_model, model_dat)\n",
    "            except CouldNotFitException:\n",
    "                continue\n",
    "            agents += 1\n",
    "            results.append([float(v) for v in fitted_params.values()] + [\n",
    "                spc.get_stay_prob(1, 1),\n",
    "                spc.get_stay_prob(1, 0),\n",
    "                spc.get_stay_prob(0, 1),\n",
    "                spc.get_stay_prob(0, 0),\n",
    "            ])\n",
    "        if print_progress:\n",
    "            print()\n",
    "        model_results[model] = pd.DataFrame(results, columns=cols)\n",
    "    return model_results\n",
    "\n",
    "# For the paper, 1000 agents of each type were simulated rather than 100\n",
    "results = simulate_mb_agents(num_agents=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the results: (1) the idealized stay probabilities for a model-free, correct model-based, and hybrid agents, (2) the stay probabilities for the simulated unlucky symbol and TDLR agents, and (3) the model-based agents for the three types of model-based agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Color for the stay probabilities after rare transitions (red)\n",
    "COLOR_RARE = (0.8941176470588236, 0.10196078431372549, 0.10980392156862745, 1.0)\n",
    "# Color for the stay probabilities after common transitions (blue)\n",
    "COLOR_COMMON = (0.21568627450980393, 0.49411764705882355, 0.7215686274509804, 1.0)\n",
    "\n",
    "def plot_results(model_results, histogram_bins=None):\n",
    "    \"\"\"Plot results from simulated model-based agents in comparison with idealized results.\"\"\"\n",
    "    plt.figure(figsize=(3*4, 2*4))\n",
    "    panel_label = ord('A')\n",
    "    def draw_label(ax, panel_label):\n",
    "        ax.text(-0.15, 1.2, chr(panel_label), transform=ax.transAxes, fontsize=16, va='top', ha='right')\n",
    "        return panel_label + 1\n",
    "    models_coefs = ( # These numbers are from Decker et al. (2016)\n",
    "        ('Model-free', [0.9205614508160216, 0.9205614508160216, 0.7772998611746911, 0.7772998611746911]),\n",
    "        ('Correct model-based', [0.9205614508160216, 0.7772998611746911, 0.7772998611746911, 0.9205614508160216]),\n",
    "        ('Hybrid', [0.9478464369215823, 0.8721384336809187, 0.6899744811276125, 0.8556968659094812]),\n",
    "    )\n",
    "    for i, (condition, y) in enumerate(models_coefs):\n",
    "        # Plot idealized stay probabilities\n",
    "        ax = plt.subplot(2, 3, i + 1)\n",
    "        panel_label = draw_label(ax, panel_label)\n",
    "        plt.title(condition)\n",
    "        plt.ylim(0.5, 1)\n",
    "        plt.bar((0, 2), (y[0], y[2]), color=COLOR_COMMON, label='Common')\n",
    "        plt.bar((1, 3), (y[1], y[3]), color=COLOR_RARE, label='Rare')\n",
    "        plt.xticks((0.5, 2.5), ('Rewarded', 'Unrewarded'))\n",
    "        plt.yticks([])\n",
    "        plt.xlabel('Previous outcome')\n",
    "        plt.ylabel('Stay probability')\n",
    "        if i == 0:\n",
    "            plt.legend(loc='best', title='Previous transition')\n",
    "    for plotnum, model in enumerate((mb_unlucky_symbol, mb_tdlr)):\n",
    "        # Plot stay probabilities\n",
    "        ax = plt.subplot(2, 3, plotnum + 4)\n",
    "        panel_label = draw_label(ax, panel_label)\n",
    "        plt.title(MODEL_LABELS[model])\n",
    "        plt.xlabel('Previous outcome')\n",
    "        plt.ylabel('Stay probability')\n",
    "        plt.ylim(0, 1)\n",
    "        y = [\n",
    "            model_results[model].stay_prob_11.mean(),\n",
    "            model_results[model].stay_prob_10.mean(),\n",
    "            model_results[model].stay_prob_01.mean(),\n",
    "            model_results[model].stay_prob_00.mean(),\n",
    "        ]\n",
    "        plt.bar((0, 2), (y[0], y[2]), color=COLOR_COMMON, label='Common')\n",
    "        plt.bar((1, 3), (y[1], y[3]), color=COLOR_RARE, label='Rare')\n",
    "        plt.xticks((0.5, 2.5), ('Rewarded', 'Unrewarded'))\n",
    "        if plotnum == 0:\n",
    "            plt.legend(title='Previous transition')\n",
    "    # Plot histogram of weights\n",
    "    ax = plt.subplot(2, 3, 6)\n",
    "    panel_label = draw_label(ax, panel_label)\n",
    "    plt.title('Hybrid model fitting')\n",
    "    plt.ylabel('Relative frequency')\n",
    "    plt.xlabel('Model-based weight')\n",
    "    plt.xlim(0, 1)\n",
    "    for plotnum, model in enumerate(MODELS_PARAMS.keys()):\n",
    "        plt.hist(model_results[model].w, bins=histogram_bins, density=True, label=MODEL_LABELS[model], alpha=0.75)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_results(results, histogram_bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
